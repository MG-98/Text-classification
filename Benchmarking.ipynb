{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file content</th>\n",
       "      <th>class number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As summer the usual are being made Me was thin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Southern 41493 Please send me any to this ride...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 Trying to figure out a way to put a halogen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In article Frank Ball wrote The 400 model is e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric Nelson My 83 Nighthawk two related with t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18743</th>\n",
       "      <td>12GB hard Drive Brand NEW with full factory wa...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18744</th>\n",
       "      <td>my 14 monitor id dead due to the failure if yo...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18745</th>\n",
       "      <td>selling 388 worth of for 100 or Ill split it i...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18746</th>\n",
       "      <td>Computer card good for doing graphics on your ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18747</th>\n",
       "      <td>COMPUTER HARDWARE all are working fine last ti...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18707 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file content  class number\n",
       "0      As summer the usual are being made Me was thin...             0\n",
       "1      Southern 41493 Please send me any to this ride...             0\n",
       "2      1 Trying to figure out a way to put a halogen ...             0\n",
       "3      In article Frank Ball wrote The 400 model is e...             0\n",
       "4      Eric Nelson My 83 Nighthawk two related with t...             0\n",
       "...                                                  ...           ...\n",
       "18743  12GB hard Drive Brand NEW with full factory wa...            19\n",
       "18744  my 14 monitor id dead due to the failure if yo...            19\n",
       "18745  selling 388 worth of for 100 or Ill split it i...            19\n",
       "18746  Computer card good for doing graphics on your ...            19\n",
       "18747  COMPUTER HARDWARE all are working fine last ti...            19\n",
       "\n",
       "[18707 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "Data = pandas.read_csv('Cleaned_Data.csv')\n",
    "Data = Data.drop(['Unnamed: 0'] , axis =1)\n",
    "Data = Data.dropna()\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(Data['file content'], Data['class number'])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "cv = CountVectorizer( token_pattern=r'\\b[^\\d\\W]{2,}\\b', stop_words='english')\n",
    "cv.fit(Data['file content'])\n",
    "\n",
    "traiv_cv =  cv.transform(train_x)\n",
    "test_cv =  cv.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tf-idf on the word level\n",
    "tfidf_word_level = TfidfVectorizer(token_pattern=r'\\b[^\\d\\W]{2,}\\b', stop_words='english', max_features=5000)\n",
    "tfidf_word_level.fit(Data['file content'])\n",
    "\n",
    "train_tfidf_word_level =  tfidf_word_level.transform(train_x)\n",
    "test_tfidf_word_level  =  tfidf_word_level.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tf-idf on the n-gram level\n",
    "tfidf_ngram_level = TfidfVectorizer(token_pattern=r'\\b[^\\d\\W]{2,}\\b', stop_words='english', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_ngram_level.fit(Data['file content'])\n",
    "\n",
    "train_tfidf_ngram_level =  tfidf_ngram_level.transform(train_x)\n",
    "test_tfidf_ngram_level =  tfidf_ngram_level.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mg/miniconda3/envs/NLP/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:502: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# Perform tf-idf on the n-gram level\n",
    "tfidf_char_ngram_level = TfidfVectorizer(analyzer='char' , stop_words='english', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_char_ngram_level.fit(Data['file content'])\n",
    "\n",
    "train_tfidf_char_ngram_level =  tfidf_char_ngram_level.transform(train_x)\n",
    "test_tfidf_char_ngram_level =  tfidf_char_ngram_level.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "We will use the following models :\n",
    "\n",
    "    1.Naive Bayes Classifier\n",
    "    2.Linear Classifier\n",
    "    3.Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(classifier, feature_vector_train, label,feature_vector_test , test_label ):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label )\n",
    "    \n",
    "\n",
    "    # predict the labels on test dataset\n",
    "    test_predictions = classifier.predict(feature_vector_test)\n",
    "    TestAccuracy =metrics.accuracy_score(test_predictions, test_label)\n",
    "   \n",
    "    ConfusionMatrix = confusion_matrix(test_label, test_predictions)\n",
    "    PrecisionScore = precision_score(test_label, test_predictions ,average='macro')\n",
    "    RecallScore = recall_score(test_label, test_predictions ,average='micro')\n",
    "    \n",
    "    return TestAccuracy , PrecisionScore , RecallScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mg/miniconda3/envs/NLP/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# NB & CV\n",
    "metrics_cv = train_test(naive_bayes.MultinomialNB(), traiv_cv, train_y, test_cv , test_y)\n",
    "\n",
    "# NB & tf-idf on the word level\n",
    "metrics_tfidf_word_level = train_test(naive_bayes.MultinomialNB(), train_tfidf_word_level, train_y, test_tfidf_word_level , test_y)\n",
    "\n",
    "# NB & tf-idf on the n-gram level\n",
    "metrics_tfidf_ngram_level = train_test(naive_bayes.MultinomialNB(), train_tfidf_ngram_level, train_y, test_tfidf_ngram_level , test_y)\n",
    "\n",
    "# NB & tf-idf on the n-gram level\n",
    "metrics_tfidf_char_ngram_level = train_test(naive_bayes.MultinomialNB(), train_tfidf_char_ngram_level, train_y, test_tfidf_char_ngram_level , test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.77228992944195, 0.7766772529593127, 0.77228992944195)\n",
      "(0.7577506948898867, 0.772897143459842, 0.7577506948898867)\n",
      "(0.5467179816121446, 0.5917871188589537, 0.5467179816121446)\n",
      "(0.6053025443660466, 0.6460662386906222, 0.6053025443660466)\n"
     ]
    }
   ],
   "source": [
    "print(metrics_cv)\n",
    "print(metrics_tfidf_word_level)\n",
    "print(metrics_tfidf_ngram_level)\n",
    "print(metrics_tfidf_char_ngram_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mg/miniconda3/envs/NLP/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mg/miniconda3/envs/NLP/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mg/miniconda3/envs/NLP/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mg/miniconda3/envs/NLP/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# LC & CV\n",
    "metrics_cv = train_test(linear_model.LogisticRegression(), traiv_cv, train_y, test_cv , test_y)\n",
    "\n",
    "# LC & tf-idf on the word level\n",
    "metrics_tfidf_word_level = train_test(linear_model.LogisticRegression(), train_tfidf_word_level, train_y, test_tfidf_word_level , test_y)\n",
    "\n",
    "# LC & tf-idf on the n-gram level\n",
    "metrics_tfidf_ngram_level = train_test(linear_model.LogisticRegression(), train_tfidf_ngram_level, train_y, test_tfidf_ngram_level , test_y)\n",
    "\n",
    "# LC & tf-idf on the n-gram level\n",
    "metrics_tfidf_char_ngram_level = train_test(linear_model.LogisticRegression(), train_tfidf_char_ngram_level, train_y, test_tfidf_char_ngram_level , test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7598888176181313, 0.7595466493138123, 0.7598888176181313)\n",
      "(0.7757109258071413, 0.7732988742563813, 0.7757109258071413)\n",
      "(0.5531323497968783, 0.5895820675899827, 0.5531323497968783)\n",
      "(0.6953175112251443, 0.6927896746815988, 0.6953175112251443)\n"
     ]
    }
   ],
   "source": [
    "print(metrics_cv)\n",
    "print(metrics_tfidf_word_level)\n",
    "print(metrics_tfidf_ngram_level)\n",
    "print(metrics_tfidf_char_ngram_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM & CV\n",
    "metrics_cv = train_test(svm.SVC(), traiv_cv, train_y, test_cv , test_y)\n",
    "\n",
    "# SVM & tf-idf on the word level\n",
    "metrics_tfidf_word_level = train_test(svm.SVC(), train_tfidf_word_level, train_y, test_tfidf_word_level , test_y)\n",
    "\n",
    "# SVM & tf-idf on the n-gram level\n",
    "metrics_tfidf_ngram_level = train_test(svm.SVC(), train_tfidf_ngram_level, train_y, test_tfidf_ngram_level , test_y)\n",
    "\n",
    "# SVM & tf-idf on the n-gram level\n",
    "metrics_tfidf_char_ngram_level = train_test(svm.SVC(), train_tfidf_char_ngram_level, train_y, test_tfidf_char_ngram_level , test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNeighborsClassifier & CV\n",
    "metrics_cv = train_test(KNeighborsClassifier(), traiv_cv, train_y, test_cv , test_y)\n",
    "\n",
    "# KNeighborsClassifier & tf-idf on the word level\n",
    "metrics_tfidf_word_level = train_test(KNeighborsClassifier(), train_tfidf_word_level, train_y, test_tfidf_word_level , test_y)\n",
    "\n",
    "# KNeighborsClassifier & tf-idf on the n-gram level\n",
    "metrics_tfidf_ngram_level = train_test(KNeighborsClassifier(), train_tfidf_ngram_level, train_y, test_tfidf_ngram_level , test_y)\n",
    "\n",
    "# KNeighborsClassifier & tf-idf on the n-gram level\n",
    "metrics_tfidf_char_ngram_level = train_test(KNeighborsClassifier(), train_tfidf_char_ngram_level, train_y, test_tfidf_char_ngram_level , test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4111610006414368, 0.5389636427364759, 0.4111610006414368)\n",
      "(0.3068206115031003, 0.7442283979643646, 0.3068206115031003)\n",
      "(0.3639084883472311, 0.41836890254760706, 0.3639084883472311)\n",
      "(0.5116527688689331, 0.6151275917099144, 0.5116527688689331)\n"
     ]
    }
   ],
   "source": [
    "print(metrics_cv)\n",
    "print(metrics_tfidf_word_level)\n",
    "print(metrics_tfidf_ngram_level)\n",
    "print(metrics_tfidf_char_ngram_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF & CV\n",
    "metrics_cv = train_test(ensemble.RandomForestClassifier(), traiv_cv, train_y, test_cv , test_y)\n",
    "\n",
    "# RF & tf-idf on the word level\n",
    "metrics_tfidf_word_level = train_test(ensemble.RandomForestClassifier(), train_tfidf_word_level, train_y, test_tfidf_word_level , test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7237545435107975, 0.7344042704116588, 0.7237545435107975)\n",
      "(0.6968141971349155, 0.6995387038626728, 0.6968141971349155)\n"
     ]
    }
   ],
   "source": [
    "print(metrics_cv)\n",
    "print(metrics_tfidf_word_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mg/miniconda3/envs/NLP/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:00:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mg/miniconda3/envs/NLP/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:00:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# XGBoost & CV\n",
    "metrics_cv = train_test(xgboost.XGBClassifier(), traiv_cv.tocsc(), train_y, test_cv.tocsc() , test_y)\n",
    "\n",
    "# XGBoost & tf-idf on the word level\n",
    "metrics_tfidf_word_level = train_test(xgboost.XGBClassifier(), train_tfidf_word_level.tocsc(), train_y, test_tfidf_word_level.tocsc() , test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7258926662390421, 0.7245284226994679, 0.7258926662390421)\n",
      "(0.7055805003207184, 0.7014430990396145, 0.7055805003207184)\n"
     ]
    }
   ],
   "source": [
    "print(metrics_cv)\n",
    "print(metrics_tfidf_word_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
